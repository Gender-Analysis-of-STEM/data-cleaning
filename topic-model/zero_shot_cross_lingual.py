# -*- coding: utf-8 -*-
"""zero-shot-cross-lingual.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1017sr8pqkIJD6SapxmzWIRSSf9zEu9WB

# Zero Cross-lingual Topic Modeling

### Fashionable to Be Dumb? A Gender Analysis of STEM Discourse in Latin American Social Media

We are going to use our Zero-Shot Topic Model to get the topics out of a collections of articles you will upload here. Then, we are going to predict the topics of unseen documents in an unseen language exploiting the multilingual capabilities of Multilingual BERT.

### Install libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install contextualized-topic-models

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pandas==1.2.5

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pyldavis

"""### Import libraries"""

from contextualized_topic_models.models.ctm import ZeroShotTM
from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation
from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing
import nltk

import pandas as pd

"""### Load Data"""

ds_esp = pd.read_csv("/content/ds_esp_2019_2021.csv")

ds_esp["final_tweet"] = ds_esp["final_tweet"].astype(str)

"""### Preprocessing

We need text without punctuation to build the bag of word. Also, we might want only to have the most frequent words inside the BoW. Too many words might not help.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# nltk.download('stopwords')

documents = ds_esp["final_tweet"].to_numpy().flatten()
sp = WhiteSpacePreprocessing(documents, stopwords_language=['spanish','english'])
preprocessed_documents, unpreprocessed_corpus, vocab = sp.preprocess()

preprocessed_documents[:2]
#unpreprocessed_corpus[:2]
vocab[:10]

"""### Export to be used by OCTIS

#### Corpus
"""

with open('corpus.csv', 'w') as f:
    for item in preprocessed_documents:
        f.write("%s\n" % item)

"""#### Vocabulary"""

vocab = open("vocabulary.txt", "r")
#print(vocab.read()) 
type(vocab)

preprocessed_documents = pd.read_csv("/content/corpus.csv")
preprocessed_documents

"""### Topic Model Preparation"""

tp = TopicModelDataPreparation("distiluse-base-multilingual-cased")
#tp = TopicModelDataPreparation("paraphrase-multilingual-mpnet-base-v2")

training_dataset = tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)

with open('vocabulary_fit2.txt', 'w') as f:
    for item in vocab:
        f.write("%s\n" % item)

len(tp.vocab)

"""### Training our Zero-Shot Contextualized Topic Model"""

vocab = pd.read_csv("/content/vocab_fit.txt")
len(vocab)

ctm = ZeroShotTM(bow_size=len(tp.vocab),
                 contextual_size=512, 
                 n_components=10, 
                 num_epochs=30,
                 model_type = "prodLDA",
                 activation = "softplus",
                 dropout = 0.6528626452408228,
                 hidden_sizes=(100,100,100)
                 )
ctm.fit(training_dataset) # run the model

"""### Topics"""

ctm.get_topic_lists(10)
df_topics = pd.DataFrame(ctm.get_topic_lists(30))
df_topics.to_csv("Words_per_Topic_30.csv", index = False)

"""### Visualization"""

lda_vis_data = ctm.get_ldavis_data_format(tp.vocab, training_dataset, n_samples=10)

import pyLDAvis as vis
stem_pd = vis.prepare(**lda_vis_data)
vis.display(stem_pd)

vis.save_html(stem_pd, 'ctm_optimizado2.html')

"""### Topic Prediction"""

topics_predictions = ctm.get_thetas(training_dataset, n_samples=10) # get all the topic predictions

topics1 = ctm.get_predicted_topics(dataset=training_dataset, n_samples=10)
topics2 = ctm.get_topic_lists()

topics3 = ctm.get_doc_topic_distribution(training_dataset)

topic4 = ctm.get_most_likely_topic(topics3)

print(len(preprocessed_documents))
print(len(unpreprocessed_corpus))
d = {'Tweet':unpreprocessed_corpus,'Topic':topics1}
df = pd.DataFrame(d)
df

"""### Merge Dataset"""

df['Topic_distribution'] = topics3.tolist()
df

df['topic_labels'] = df["Topic"].apply(lambda row: ctm.get_topic_lists(10)[row-1])
df

df.to_csv("Topic_database_topic_prediction.csv", index = False)

df_empty = pd.DataFrame({'Topic_distribution' : []})

df_empty['Topic_distribution'] = topics3.tolist()
df_empty.to_csv("Topic_distribution.csv", index=False)

"""## Imports

### Unseen languages
"""

df_pt = pd.read_csv("ds_pt_2019_2021.csv")
df_pt

portuguese_documents = df_pt["final_tweet"].to_numpy().flatten()
portuguese_documents

tp = TopicModelDataPreparation("distiluse-base-multilingual-cased")

testing_dataset = tp.transform(portuguese_documents) # create dataset for the testset

"""### Topic Prediction"""

# n_sample how many times to sample the distribution (see the documentation)
portuguese_topics_predictions = ctm.get_thetas(testing_dataset, n_samples=10) # get all the topic predictions

import numpy as np

topic_number = np.argmax(portuguese_topics_predictions[0]) # get the topic id of the first document
ctm.get_topic_lists(10)[topic_number]

import numpy as np

aux = []

for elements in portuguese_topics_predictions:
  aux.append(np.argmax(elements))
len(aux)

topics7 = ctm.get_predicted_topics(dataset=testing_dataset, n_samples=10)
topics8 = ctm.get_topic_lists()

topics9 = ctm.get_doc_topic_distribution(testing_dataset)

topics9

d = {'Tweet':portuguese_documents,'Topic':aux}
df_p = pd.DataFrame(d)
df_p

df_p['topic_labels'] = df_p["Topic"].apply(lambda row: ctm.get_topic_lists(10)[row - 1])

## Merge topic prediction
df_p['Topic_distribution'] = topics9.tolist()
df_p

df_p.to_csv("Topic_database_topic_prediction_pt.csv", index = False)

df_empty2 = pd.DataFrame({'Topic_distribution' : []})

df_empty2['Topic_distribution'] = topics9.tolist()
df_empty2
df_empty2.to_csv("Topic_distribution_pt.csv", index=False)

df_empty2

"""### Save Model"""

ctm.save(models_dir="./")

"""## Visualizations"""

ctm.get_wordcloud(9,n_words=40,background_color="white")

tp_w_m = ctm.get_topic_word_matrix()
df_tp = pd.DataFrame(tp_w_m)
df_tp.to_csv("Topic_word_matrix.csv", index = False)

topics = ctm.get_topics()
d = {'Topics':topics}
topics_df = pd.DataFrame(d)
topics_df.to_csv("Topics.csv", index=False)

ctm.get_most_likely_topic(doc_topic_distribution=topics3)

import json
res = json.load(open("result.json",'r'))
res.keys()

res['x_iters']['activation']

res["f_val"]

import matplotlib.pyplot as plt

plt.plot(res["f_val"])

"""### Load Model"""

ctm = ZeroShotTM(bow_size=len(content_list),
                 contextual_size=512, 
                 n_components=10, 
                 num_epochs=30,
                 model_type = "prodLDA",
                 activation = "softplus",
                 dropout = 0.6528626452408228,
                 hidden_sizes=(100,100,100)
                 )

ctm.load("/content/test", epoch=29)

my_file = open("vocabulary.txt", "r")

content = my_file.read()

content_list = content.split("\n")

my_file.close()

print(content_list)

len(content_list)